INFERENCE_OUTPUT = "inference_outputs"
EVALUATION_OUTPUT = "evaluation_outputs"
RESULT_OUTPUT = "results"

MODELS = [
    "GPT-3.5-Turbo-16K",
    "Llama-2-chat-7B",
    "Llama-2-chat-13B",
    "Llama-2-chat-70B",
    "Vicuna-13B-16K-v1.5",
    "ChatGLM2-6B-32K",
    "ChatGLM3-6B-32K",
    "MistralLite-7B-32K",
    "Zephyr-7b-beta",
]

DATASET_MAP = {
    "mnds-news": "MN-DS",
    "mnds-news_autoregressive": "MN-DS (wo gold)",
    # "mnds-news-retrieval_autoregressive": "MN-DS Retrieval (wo gold)",
    "newsqa": "NewsQA",
    "newsqa_autoregressive": "NewsQA (wo gold)",
    "tedtalks": "TED-talks",
    "news_commentary": "News-Commentary",
    "cnnnews": "CNN",
    "multi-task-single-inst_qa": "Multi-Task (QA)",
    # "multi-task_qa": "QA",
    "multi-task-single-inst_sum": "Multi-Task (SUM)",
    # "multi-task_sum": "SUM",
    "multi-task-single-inst_trans": "Multi-Task (TRANS)",
    # "multi-task_trans": "TRANS",
    "multi-task-single-inst_ner": "Multi-Task (NER)",
    # "multi-task_ner": "NER",
    "multi-task-single-inst_pos": "Multi-Task (POS)",
    # "multi-task_pos": "POS",
    "multi-task-single-inst_cls": "Multi-Task (CLS)",
    # "multi-task_cls": "CLS",
    "multi-task-single-inst_rel": "Multi-Task (REL)",
    # "multi-task_rel": "REL",
    "multi-task-multi-inst-multi-turn": "Multi-Task Multi-Inst Multi-Turn",
    "multi-task-multi-inst-single-turn": "Multi-Task Multi-Inst Single-Turn",
    "multi-task-single-inst-multi-turn": "Multi-Task Single-Inst Multi-Turn",
    "multi-task-single-inst-single-turn": "Multi-Task Single-Inst Single-Turn",
    "cyrus-qa": "QA",
    "cyrus-qa_autoregressive": "QA (wo gold)",
    "cyrus-cls": "CLS",
    "cyrus-cls_autoregressive": "CLS (wo gold)",
}

MODEL_MAP = {
    # "gpt-3.5-turbo-16k": "GPT-3.5-Turbo-16k",
    "gpt-3.5-turbo": "GPT-3.5-Turbo",
    # "chatglm2-6b-32k": "ChatGLM2-6B-32K",
    "chatglm3-6b-32k": "ChatGLM3-6B-32K",
    # "vicuna-13b-16k": "Vicuna-13B-16K-v1.5",
    "vicuna-7b": "Vicuna-7B-v1.5",
    "vicuna-13b": "Vicuna-13B-v1.5",
    "llama2-chat-7b": "Llama-2-chat-7B",
    "llama2-chat-13b": "Llama-2-chat-13B",
    "llama2-chat-70b": "Llama-2-chat-70B",
    # "mistrallite": "MistralLite-7B-32K",
    # "zephyr-7b-beta": "Zephyr-7B",
    # "openchat-3.5": "OpenChat-3.5-7B",
    # "openhermes-2.5": "OpenHermes-2.5-7B",
    # "starling-7b": "Starling-7B",
    "qwen-chat-7b": "Qwen-chat-7B",
    "qwen-chat-14b": "Qwen-chat-14B",
    "qwen-chat-72b": "Qwen-chat-72B",
}

END_TOKENS = {
    "chatglm3-6b-32k": ["<|user|>"],
    "chatglm2-6b-32k": ["问："],
    "llama2-chat-7b": [
        "</s>",
    ],
    "llama2-chat-13b": ["</s>"],
    "llama2-chat-70b": ["</s>"],
    "vicuna-13b-16k": ["</s>", "USER:"],
    "vicuna-7b": ["</s>", "USER:"],
    "vicuna-13b": ["</s>", "USER:"],
    "gpt-3.5-turbo-16k": [],
    "mistrallite": ["</s>", "<|prompter|>"],
    "zephyr-7b-beta": ["</s>", "<|user|>"],
    "openchat-3.5": ["<|end_of_turn|>", "GPT4 Correct User"],
    "openhermes-2.5": ["<|im_end|>", "user\n"],
    "starling-7b": ["<|end_of_turn|>", "GPT4 Correct User"],
    "qwen-chat-7b": ["<|im_end|>"],
    "qwen-chat-14b": ["<|im_end|>"],
    "qwen-chat-72b": ["<|im_end|>"],
    "gpt-3.5-turbo": [],
    "gpt-4": [],
}


topics = [
    "ethics",
    "sports",
    "music",
    "art",
    "science",
    "literature",
    "economics",
    "medicine",
    "food",
    "technology",
    "history",
    "travel",
    "education",
]
